{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b434eaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c45624935854d378deabb321442425b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "\n",
    "conn= duckdb.connect('modelling_db')\n",
    "df = conn.execute(\"\"\"SELECT * FROM abt_estacoes_3_vizinhas WHERE YEAR(dt_medicao)=2022\"\"\").fetch_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e994ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea1b2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da766e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def smoteR(dataframe, target_column, explanatory_variables=None, \n",
    "           relevance_function=None, threshold=0.5, pct_oversampling=100, \n",
    "           pct_undersampling=100, number_of_nearest_neighbors=5, \n",
    "           constraint_columns=None, random_state=None):\n",
    "    \"\"\"\n",
    "    Implementa o algoritmo SMOTE-R para balanceamento de datasets com target contínuo.\n",
    "    \n",
    "    Parâmetros:\n",
    "    -----------\n",
    "    dataframe : pd.DataFrame\n",
    "        DataFrame original com os dados\n",
    "    target_column : str\n",
    "        Nome da coluna alvo/target\n",
    "    explanatory_variables : list, optional\n",
    "        Lista das variáveis explicativas a serem consideradas. \n",
    "        Se None, usa todas as colunas exceto target\n",
    "    relevance_function : callable, optional\n",
    "        Função que determina a relevância de uma observação.\n",
    "        Se None, usa uma função baseada na distância do percentil 50\n",
    "    threshold : float, default=0.5\n",
    "        Limiar para determinar observações raras vs comuns\n",
    "    pct_oversampling : int, default=100\n",
    "        Porcentagem de oversampling para casos raros\n",
    "    pct_undersampling : int, default=100\n",
    "        Porcentagem de undersampling para casos comuns\n",
    "    number_of_nearest_neighbors : int, default=5\n",
    "        Número de vizinhos mais próximos para geração sintética\n",
    "    constraint_columns : list or str, optional\n",
    "        Lista de colunas que devem ter valores iguais entre a amostra e seus vizinhos.\n",
    "        Pode ser uma string (coluna única) ou lista de strings (múltiplas colunas).\n",
    "        Se None, não há restrições nos vizinhos.\n",
    "        Exemplo: ['dt_medicao'] ou ['dt_medicao', 'regiao']\n",
    "    random_state : int, optional\n",
    "        Semente para reprodutibilidade\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame balanceado com casos originais raros, casos comuns \n",
    "        sub-amostrados e casos sintéticos gerados\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Configurar random state se fornecido\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "            random.seed(random_state)\n",
    "        \n",
    "        print(\"🚀 Iniciando SMOTE-R...\")\n",
    "        print(f\"   Dataset original: {len(dataframe):,} registros\")\n",
    "        \n",
    "        # Validações iniciais\n",
    "        if target_column not in dataframe.columns:\n",
    "            raise ValueError(f\"Coluna target '{target_column}' não encontrada no DataFrame\")\n",
    "        \n",
    "        if not pd.api.types.is_numeric_dtype(dataframe[target_column]):\n",
    "            raise ValueError(f\"Coluna target '{target_column}' deve ser numérica para SMOTE-R\")\n",
    "        \n",
    "        # Definir variáveis explicativas\n",
    "        if explanatory_variables is None:\n",
    "            explanatory_variables = [col for col in dataframe.columns if col != target_column]\n",
    "            print(f\"   Usando todas as {len(explanatory_variables)} features como variáveis explicativas\")\n",
    "        else:\n",
    "            # Validar se todas as variáveis explicativas existem\n",
    "            missing_vars = set(explanatory_variables) - set(dataframe.columns)\n",
    "            if missing_vars:\n",
    "                raise ValueError(f\"Variáveis explicativas não encontradas: {missing_vars}\")\n",
    "            print(f\"   Usando {len(explanatory_variables)} variáveis explicativas especificadas\")\n",
    "        \n",
    "        # Validar constraint_columns se fornecidas\n",
    "        if constraint_columns is not None:\n",
    "            if isinstance(constraint_columns, str):\n",
    "                constraint_columns = [constraint_columns]\n",
    "            \n",
    "            missing_constraints = set(constraint_columns) - set(dataframe.columns)\n",
    "            if missing_constraints:\n",
    "                raise ValueError(f\"Colunas de restrição não encontradas: {missing_constraints}\")\n",
    "            \n",
    "            print(f\"   Restrições de vizinhança: {constraint_columns}\")\n",
    "        else:\n",
    "            constraint_columns = []\n",
    "            print(\"   Nenhuma restrição de vizinhança aplicada\")\n",
    "        \n",
    "        # Definir função de relevância padrão se não fornecida\n",
    "        if relevance_function is None:\n",
    "            median_value = dataframe[target_column].median()\n",
    "            mad_value = np.median(np.abs(dataframe[target_column] - median_value))\n",
    "            \n",
    "            def default_relevance_function(x):\n",
    "                \"\"\"Função de relevância baseada na distância do percentil 50\"\"\"\n",
    "                if mad_value == 0:\n",
    "                    return 0.5  # Se MAD é 0, todos os valores são iguais\n",
    "                return abs(x - median_value) / (mad_value * 1.4826)  # Fator de escala para normalização\n",
    "            \n",
    "            relevance_function = default_relevance_function\n",
    "            print(f\"   Usando função de relevância padrão (mediana: {median_value:.3f}, MAD: {mad_value:.3f})\")\n",
    "        \n",
    "        # Etapa 1: Identificar observações raras e comuns\n",
    "        print(\"\\n🔍 Etapa 1: Identificando observações raras e comuns...\")\n",
    "        \n",
    "        with tqdm(total=len(dataframe), desc=\"Calculando relevância\") as pbar:\n",
    "            relevance_scores = []\n",
    "            for idx, value in enumerate(dataframe[target_column]):\n",
    "                try:\n",
    "                    score = relevance_function(value)\n",
    "                    relevance_scores.append(score)\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao calcular relevância para valor {value}: {e}\")\n",
    "                    relevance_scores.append(0)\n",
    "                pbar.update(1)\n",
    "        \n",
    "        # Identificar índices das observações raras e comuns\n",
    "        relevance_series = pd.Series(relevance_scores, index=dataframe.index)\n",
    "        rare_observations_indexes = dataframe.loc[relevance_series >= threshold].index\n",
    "        common_observations_indexes = dataframe.loc[relevance_series < threshold].index\n",
    "        \n",
    "        rare_df = dataframe.loc[rare_observations_indexes].copy()\n",
    "        common_df = dataframe.loc[common_observations_indexes].copy()\n",
    "        \n",
    "        print(f\"   Observações raras: {len(rare_df):,} ({len(rare_df)/len(dataframe)*100:.1f}%)\")\n",
    "        print(f\"   Observações comuns: {len(common_df):,} ({len(common_df)/len(dataframe)*100:.1f}%)\")\n",
    "        \n",
    "        if len(rare_df) == 0:\n",
    "            print(\"⚠️  Nenhuma observação rara encontrada. Retornando dataset original.\")\n",
    "            return dataframe.copy()\n",
    "        \n",
    "        if len(rare_df) < number_of_nearest_neighbors:\n",
    "            print(f\"⚠️  Ajustando número de vizinhos de {number_of_nearest_neighbors} para {len(rare_df)-1}\")\n",
    "            number_of_nearest_neighbors = max(1, len(rare_df) - 1)\n",
    "        \n",
    "        # Etapa 2: Gerar casos sintéticos para observações raras\n",
    "        print(f\"\\n🧬 Etapa 2: Gerando casos sintéticos (oversampling: {pct_oversampling}%)...\")\n",
    "        \n",
    "        try:\n",
    "            synthetic_cases = generate_synthetic_cases_with_target_smoter(\n",
    "                dataframe=rare_df,\n",
    "                target_column=target_column,\n",
    "                explanatory_variables=explanatory_variables,\n",
    "                oversampling_ratio=pct_oversampling,\n",
    "                num_neighbors=number_of_nearest_neighbors,\n",
    "                constraint_columns=constraint_columns\n",
    "            )\n",
    "            \n",
    "            print(f\"   Casos sintéticos gerados: {len(synthetic_cases):,}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erro na geração de casos sintéticos: {e}\")\n",
    "            print(\"   Continuando sem casos sintéticos...\")\n",
    "            synthetic_cases = pd.DataFrame()\n",
    "        \n",
    "        # Etapa 3: Sub-amostragem de observações comuns\n",
    "        print(f\"\\n⚖️ Etapa 3: Sub-amostragem de observações comuns (undersampling: {pct_undersampling}%)...\")\n",
    "        \n",
    "        if len(common_df) > 0:\n",
    "            # Calcular número de casos comuns a manter\n",
    "            total_rare_and_synthetic = len(rare_df) + len(synthetic_cases)\n",
    "            n_common_to_keep = int((pct_undersampling / 100) * total_rare_and_synthetic)\n",
    "            n_common_to_keep = min(n_common_to_keep, len(common_df))\n",
    "            \n",
    "            if n_common_to_keep > 0:\n",
    "                with tqdm(total=1, desc=\"Selecionando casos comuns\") as pbar:\n",
    "                    selected_common_indexes = random.sample(list(common_observations_indexes), n_common_to_keep)\n",
    "                    selected_common_df = dataframe.loc[selected_common_indexes].copy()\n",
    "                    pbar.update(1)\n",
    "                \n",
    "                print(f\"   Casos comuns selecionados: {len(selected_common_df):,} de {len(common_df):,}\")\n",
    "            else:\n",
    "                selected_common_df = pd.DataFrame()\n",
    "                print(\"   Nenhum caso comum selecionado\")\n",
    "        else:\n",
    "            selected_common_df = pd.DataFrame()\n",
    "            print(\"   Nenhuma observação comum disponível\")\n",
    "        \n",
    "        # Etapa 4: Combinar todos os casos\n",
    "        print(f\"\\n🔗 Etapa 4: Combinando datasets...\")\n",
    "        \n",
    "        datasets_to_combine = []\n",
    "        \n",
    "        # Adicionar casos raros originais\n",
    "        if len(rare_df) > 0:\n",
    "            datasets_to_combine.append(rare_df)\n",
    "            print(f\"   - Casos raros originais: {len(rare_df):,}\")\n",
    "        \n",
    "        # Adicionar casos comuns selecionados\n",
    "        if len(selected_common_df) > 0:\n",
    "            datasets_to_combine.append(selected_common_df)\n",
    "            print(f\"   - Casos comuns selecionados: {len(selected_common_df):,}\")\n",
    "        \n",
    "        # Adicionar casos sintéticos\n",
    "        if len(synthetic_cases) > 0:\n",
    "            datasets_to_combine.append(synthetic_cases)\n",
    "            print(f\"   - Casos sintéticos: {len(synthetic_cases):,}\")\n",
    "        \n",
    "        if not datasets_to_combine:\n",
    "            print(\"❌ Nenhum dataset para combinar. Retornando dataset original.\")\n",
    "            return dataframe.copy()\n",
    "        \n",
    "        # Combinar datasets\n",
    "        with tqdm(total=1, desc=\"Combinando datasets\") as pbar:\n",
    "            final_dataframe = pd.concat(datasets_to_combine, ignore_index=True)\n",
    "            pbar.update(1)\n",
    "        \n",
    "        # Etapa 5: Relatório final\n",
    "        print(f\"\\n✅ SMOTE-R concluído com sucesso!\")\n",
    "        print(f\"   Dataset original: {len(dataframe):,} registros\")\n",
    "        print(f\"   Dataset final: {len(final_dataframe):,} registros\")\n",
    "        print(f\"   Variação: {((len(final_dataframe) - len(dataframe)) / len(dataframe) * 100):+.1f}%\")\n",
    "        \n",
    "        # Estatísticas do target\n",
    "        print(f\"\\n📊 Estatísticas do target '{target_column}':\")\n",
    "        print(f\"   Original - Média: {dataframe[target_column].mean():.3f}, Mediana: {dataframe[target_column].median():.3f}\")\n",
    "        print(f\"   Final    - Média: {final_dataframe[target_column].mean():.3f}, Mediana: {final_dataframe[target_column].median():.3f}\")\n",
    "        \n",
    "        return final_dataframe\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro crítico no SMOTE-R: {e}\")\n",
    "        print(\"   Retornando dataset original...\")\n",
    "        return dataframe.copy()\n",
    "\n",
    "\n",
    "def generate_synthetic_cases_with_target_smoter(dataframe, target_column, explanatory_variables, \n",
    "                                               oversampling_ratio, num_neighbors, constraint_columns=None):\n",
    "    \"\"\"\n",
    "    Função auxiliar para gerar casos sintéticos otimizada para SMOTE-R\n",
    "    \n",
    "    Parâmetros adicionais:\n",
    "    - constraint_columns: Lista de colunas que devem ter valores iguais entre vizinhos\n",
    "    \"\"\"\n",
    "    \n",
    "    synthetic_cases = []\n",
    "    num_new_cases = int(oversampling_ratio / 100)\n",
    "    \n",
    "    if num_new_cases == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Separar features e target\n",
    "    features = dataframe[explanatory_variables].copy()\n",
    "    target = dataframe[target_column].copy()\n",
    "    \n",
    "    # Separar colunas numéricas e categóricas das features\n",
    "    numeric_columns = features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_columns = features.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Preparar dados para KNN\n",
    "    label_encoders = {}\n",
    "    encoded_features = features.copy()\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        encoded_features[col] = le.fit_transform(features[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    features_for_knn = encoded_features.values\n",
    "    knn_model = NearestNeighbors(n_neighbors=min(num_neighbors + 1, len(dataframe)))\n",
    "    knn_model.fit(features_for_knn)\n",
    "    \n",
    "    # Para cada caso existente\n",
    "    with tqdm(total=len(features), desc=\"Gerando casos sintéticos\") as pbar:\n",
    "        for case_index, (_, original_case) in enumerate(features.iterrows()):\n",
    "            \n",
    "            # Encontrar vizinhos mais próximos\n",
    "            distances, neighbor_indices = knn_model.kneighbors([features_for_knn[case_index]])\n",
    "            neighbor_indices = neighbor_indices[0][1:]  # Remove o primeiro (próprio caso)\n",
    "            \n",
    "            # Gerar novos casos\n",
    "            for _ in range(num_new_cases):\n",
    "                selected_neighbor_idx = random.choice(neighbor_indices)\n",
    "                selected_neighbor = features.iloc[selected_neighbor_idx]\n",
    "                \n",
    "                synthetic_case = {}\n",
    "                \n",
    "                # Processar features\n",
    "                for attribute_name in explanatory_variables:\n",
    "                    if attribute_name in numeric_columns:\n",
    "                        original_value = original_case[attribute_name]\n",
    "                        neighbor_value = selected_neighbor[attribute_name]\n",
    "                        difference = neighbor_value - original_value\n",
    "                        random_factor = np.random.uniform(0, 1)\n",
    "                        synthetic_value = original_value + random_factor * difference\n",
    "                        synthetic_case[attribute_name] = synthetic_value\n",
    "                    else:\n",
    "                        original_value = original_case[attribute_name]\n",
    "                        neighbor_value = selected_neighbor[attribute_name]\n",
    "                        synthetic_case[attribute_name] = random.choice([original_value, neighbor_value])\n",
    "                \n",
    "                # Calcular target usando média ponderada pelas distâncias\n",
    "                original_target = target.iloc[case_index]\n",
    "                neighbor_target = target.iloc[selected_neighbor_idx]\n",
    "                \n",
    "                # Usar distâncias para ponderar\n",
    "                case_distance = distances[0][0] if distances[0][0] > 0 else 0.001\n",
    "                neighbor_distance = distances[0][np.where(neighbor_indices == selected_neighbor_idx)[0][0] + 1]\n",
    "                \n",
    "                total_distance = case_distance + neighbor_distance\n",
    "                if total_distance > 0:\n",
    "                    weight_original = neighbor_distance / total_distance\n",
    "                    weight_neighbor = case_distance / total_distance\n",
    "                else:\n",
    "                    weight_original = weight_neighbor = 0.5\n",
    "                \n",
    "                synthetic_target = weight_original * original_target + weight_neighbor * neighbor_target\n",
    "                synthetic_case[target_column] = synthetic_target\n",
    "                \n",
    "                synthetic_cases.append(synthetic_case)\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Incluir todas as colunas originais (não apenas explicativas)\n",
    "    result_df = pd.DataFrame(synthetic_cases)\n",
    "    \n",
    "    # Adicionar colunas que não estavam nas variáveis explicativas nem target\n",
    "    missing_columns = set(dataframe.columns) - set(explanatory_variables) - {target_column}\n",
    "    for col in missing_columns:\n",
    "        # Para colunas não explicativas, usar valores da primeira linha como padrão\n",
    "        # ou implementar lógica específica conforme necessário\n",
    "        result_df[col] = dataframe[col].iloc[0]  # Simplificação - pode ser melhorado\n",
    "    \n",
    "    # Reordenar colunas para manter ordem original\n",
    "    result_df = result_df[dataframe.columns]\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d2564235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Iniciando SMOTE-R...\n",
      "   Dataset original: 834,592 registros\n",
      "   Usando 2 variáveis explicativas especificadas\n",
      "   Restrições de vizinhança: ['dt_medicao']\n",
      "\n",
      "🔍 Etapa 1: Identificando observações raras e comuns...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef153466419417b9c62930d7bcfa2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculando relevância:   0%|          | 0/834592 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Observações raras: 101,872 (12.2%)\n",
      "   Observações comuns: 732,720 (87.8%)\n",
      "\n",
      "🧬 Etapa 2: Gerando casos sintéticos (oversampling: 150%)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e4d224e15a4a8eaffc4177722a9a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gerando casos sintéticos:   0%|          | 0/101872 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Casos sintéticos gerados: 101,872\n",
      "\n",
      "⚖️ Etapa 3: Sub-amostragem de observações comuns (undersampling: 80%)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94064d7c570847a2b19f8e07c9482765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Selecionando casos comuns:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Casos comuns selecionados: 162,995 de 732,720\n",
      "\n",
      "🔗 Etapa 4: Combinando datasets...\n",
      "   - Casos raros originais: 101,872\n",
      "   - Casos comuns selecionados: 162,995\n",
      "   - Casos sintéticos: 101,872\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34248d72ef384c4fb3c81820eac56ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinando datasets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ SMOTE-R concluído com sucesso!\n",
      "   Dataset original: 834,592 registros\n",
      "   Dataset final: 366,739 registros\n",
      "   Variação: -56.1%\n",
      "\n",
      "📊 Estatísticas do target 'vl_precipitacao':\n",
      "   Original - Média: 3.944, Mediana: 0.000\n",
      "   Final    - Média: 15.159, Mediana: 11.400\n"
     ]
    }
   ],
   "source": [
    "df_balanced = smoteR(\n",
    "    dataframe=df,\n",
    "    target_column='vl_precipitacao',\n",
    "    explanatory_variables=['latitude', 'longitude'],\n",
    "    constraint_columns='dt_medicao',  # String única\n",
    "    pct_oversampling=150,\n",
    "    pct_undersampling=80,\n",
    "    number_of_nearest_neighbors=3,\n",
    "    random_state=42,\n",
    "    relevance_function=relevance_function\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
