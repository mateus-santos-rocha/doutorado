{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b434eaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c45624935854d378deabb321442425b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "\n",
    "conn= duckdb.connect('modelling_db')\n",
    "df = conn.execute(\"\"\"SELECT * FROM abt_estacoes_3_vizinhas WHERE YEAR(dt_medicao)=2022\"\"\").fetch_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e994ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea1b2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da766e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def smoteR(dataframe, target_column, explanatory_variables=None, \n",
    "           relevance_function=None, threshold=0.5, pct_oversampling=100, \n",
    "           pct_undersampling=100, number_of_nearest_neighbors=5, \n",
    "           constraint_columns=None, random_state=None):\n",
    "    \"\"\"\n",
    "    Implementa o algoritmo SMOTE-R para balanceamento de datasets com target cont√≠nuo.\n",
    "    \n",
    "    Par√¢metros:\n",
    "    -----------\n",
    "    dataframe : pd.DataFrame\n",
    "        DataFrame original com os dados\n",
    "    target_column : str\n",
    "        Nome da coluna alvo/target\n",
    "    explanatory_variables : list, optional\n",
    "        Lista das vari√°veis explicativas a serem consideradas. \n",
    "        Se None, usa todas as colunas exceto target\n",
    "    relevance_function : callable, optional\n",
    "        Fun√ß√£o que determina a relev√¢ncia de uma observa√ß√£o.\n",
    "        Se None, usa uma fun√ß√£o baseada na dist√¢ncia do percentil 50\n",
    "    threshold : float, default=0.5\n",
    "        Limiar para determinar observa√ß√µes raras vs comuns\n",
    "    pct_oversampling : int, default=100\n",
    "        Porcentagem de oversampling para casos raros\n",
    "    pct_undersampling : int, default=100\n",
    "        Porcentagem de undersampling para casos comuns\n",
    "    number_of_nearest_neighbors : int, default=5\n",
    "        N√∫mero de vizinhos mais pr√≥ximos para gera√ß√£o sint√©tica\n",
    "    constraint_columns : list or str, optional\n",
    "        Lista de colunas que devem ter valores iguais entre a amostra e seus vizinhos.\n",
    "        Pode ser uma string (coluna √∫nica) ou lista de strings (m√∫ltiplas colunas).\n",
    "        Se None, n√£o h√° restri√ß√µes nos vizinhos.\n",
    "        Exemplo: ['dt_medicao'] ou ['dt_medicao', 'regiao']\n",
    "    random_state : int, optional\n",
    "        Semente para reprodutibilidade\n",
    "        \n",
    "    Retorna:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame balanceado com casos originais raros, casos comuns \n",
    "        sub-amostrados e casos sint√©ticos gerados\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Configurar random state se fornecido\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "            random.seed(random_state)\n",
    "        \n",
    "        print(\"üöÄ Iniciando SMOTE-R...\")\n",
    "        print(f\"   Dataset original: {len(dataframe):,} registros\")\n",
    "        \n",
    "        # Valida√ß√µes iniciais\n",
    "        if target_column not in dataframe.columns:\n",
    "            raise ValueError(f\"Coluna target '{target_column}' n√£o encontrada no DataFrame\")\n",
    "        \n",
    "        if not pd.api.types.is_numeric_dtype(dataframe[target_column]):\n",
    "            raise ValueError(f\"Coluna target '{target_column}' deve ser num√©rica para SMOTE-R\")\n",
    "        \n",
    "        # Definir vari√°veis explicativas\n",
    "        if explanatory_variables is None:\n",
    "            explanatory_variables = [col for col in dataframe.columns if col != target_column]\n",
    "            print(f\"   Usando todas as {len(explanatory_variables)} features como vari√°veis explicativas\")\n",
    "        else:\n",
    "            # Validar se todas as vari√°veis explicativas existem\n",
    "            missing_vars = set(explanatory_variables) - set(dataframe.columns)\n",
    "            if missing_vars:\n",
    "                raise ValueError(f\"Vari√°veis explicativas n√£o encontradas: {missing_vars}\")\n",
    "            print(f\"   Usando {len(explanatory_variables)} vari√°veis explicativas especificadas\")\n",
    "        \n",
    "        # Validar constraint_columns se fornecidas\n",
    "        if constraint_columns is not None:\n",
    "            if isinstance(constraint_columns, str):\n",
    "                constraint_columns = [constraint_columns]\n",
    "            \n",
    "            missing_constraints = set(constraint_columns) - set(dataframe.columns)\n",
    "            if missing_constraints:\n",
    "                raise ValueError(f\"Colunas de restri√ß√£o n√£o encontradas: {missing_constraints}\")\n",
    "            \n",
    "            print(f\"   Restri√ß√µes de vizinhan√ßa: {constraint_columns}\")\n",
    "        else:\n",
    "            constraint_columns = []\n",
    "            print(\"   Nenhuma restri√ß√£o de vizinhan√ßa aplicada\")\n",
    "        \n",
    "        # Definir fun√ß√£o de relev√¢ncia padr√£o se n√£o fornecida\n",
    "        if relevance_function is None:\n",
    "            median_value = dataframe[target_column].median()\n",
    "            mad_value = np.median(np.abs(dataframe[target_column] - median_value))\n",
    "            \n",
    "            def default_relevance_function(x):\n",
    "                \"\"\"Fun√ß√£o de relev√¢ncia baseada na dist√¢ncia do percentil 50\"\"\"\n",
    "                if mad_value == 0:\n",
    "                    return 0.5  # Se MAD √© 0, todos os valores s√£o iguais\n",
    "                return abs(x - median_value) / (mad_value * 1.4826)  # Fator de escala para normaliza√ß√£o\n",
    "            \n",
    "            relevance_function = default_relevance_function\n",
    "            print(f\"   Usando fun√ß√£o de relev√¢ncia padr√£o (mediana: {median_value:.3f}, MAD: {mad_value:.3f})\")\n",
    "        \n",
    "        # Etapa 1: Identificar observa√ß√µes raras e comuns\n",
    "        print(\"\\nüîç Etapa 1: Identificando observa√ß√µes raras e comuns...\")\n",
    "        \n",
    "        with tqdm(total=len(dataframe), desc=\"Calculando relev√¢ncia\") as pbar:\n",
    "            relevance_scores = []\n",
    "            for idx, value in enumerate(dataframe[target_column]):\n",
    "                try:\n",
    "                    score = relevance_function(value)\n",
    "                    relevance_scores.append(score)\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao calcular relev√¢ncia para valor {value}: {e}\")\n",
    "                    relevance_scores.append(0)\n",
    "                pbar.update(1)\n",
    "        \n",
    "        # Identificar √≠ndices das observa√ß√µes raras e comuns\n",
    "        relevance_series = pd.Series(relevance_scores, index=dataframe.index)\n",
    "        rare_observations_indexes = dataframe.loc[relevance_series >= threshold].index\n",
    "        common_observations_indexes = dataframe.loc[relevance_series < threshold].index\n",
    "        \n",
    "        rare_df = dataframe.loc[rare_observations_indexes].copy()\n",
    "        common_df = dataframe.loc[common_observations_indexes].copy()\n",
    "        \n",
    "        print(f\"   Observa√ß√µes raras: {len(rare_df):,} ({len(rare_df)/len(dataframe)*100:.1f}%)\")\n",
    "        print(f\"   Observa√ß√µes comuns: {len(common_df):,} ({len(common_df)/len(dataframe)*100:.1f}%)\")\n",
    "        \n",
    "        if len(rare_df) == 0:\n",
    "            print(\"‚ö†Ô∏è  Nenhuma observa√ß√£o rara encontrada. Retornando dataset original.\")\n",
    "            return dataframe.copy()\n",
    "        \n",
    "        if len(rare_df) < number_of_nearest_neighbors:\n",
    "            print(f\"‚ö†Ô∏è  Ajustando n√∫mero de vizinhos de {number_of_nearest_neighbors} para {len(rare_df)-1}\")\n",
    "            number_of_nearest_neighbors = max(1, len(rare_df) - 1)\n",
    "        \n",
    "        # Etapa 2: Gerar casos sint√©ticos para observa√ß√µes raras\n",
    "        print(f\"\\nüß¨ Etapa 2: Gerando casos sint√©ticos (oversampling: {pct_oversampling}%)...\")\n",
    "        \n",
    "        try:\n",
    "            synthetic_cases = generate_synthetic_cases_with_target_smoter(\n",
    "                dataframe=rare_df,\n",
    "                target_column=target_column,\n",
    "                explanatory_variables=explanatory_variables,\n",
    "                oversampling_ratio=pct_oversampling,\n",
    "                num_neighbors=number_of_nearest_neighbors,\n",
    "                constraint_columns=constraint_columns\n",
    "            )\n",
    "            \n",
    "            print(f\"   Casos sint√©ticos gerados: {len(synthetic_cases):,}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro na gera√ß√£o de casos sint√©ticos: {e}\")\n",
    "            print(\"   Continuando sem casos sint√©ticos...\")\n",
    "            synthetic_cases = pd.DataFrame()\n",
    "        \n",
    "        # Etapa 3: Sub-amostragem de observa√ß√µes comuns\n",
    "        print(f\"\\n‚öñÔ∏è Etapa 3: Sub-amostragem de observa√ß√µes comuns (undersampling: {pct_undersampling}%)...\")\n",
    "        \n",
    "        if len(common_df) > 0:\n",
    "            # Calcular n√∫mero de casos comuns a manter\n",
    "            total_rare_and_synthetic = len(rare_df) + len(synthetic_cases)\n",
    "            n_common_to_keep = int((pct_undersampling / 100) * total_rare_and_synthetic)\n",
    "            n_common_to_keep = min(n_common_to_keep, len(common_df))\n",
    "            \n",
    "            if n_common_to_keep > 0:\n",
    "                with tqdm(total=1, desc=\"Selecionando casos comuns\") as pbar:\n",
    "                    selected_common_indexes = random.sample(list(common_observations_indexes), n_common_to_keep)\n",
    "                    selected_common_df = dataframe.loc[selected_common_indexes].copy()\n",
    "                    pbar.update(1)\n",
    "                \n",
    "                print(f\"   Casos comuns selecionados: {len(selected_common_df):,} de {len(common_df):,}\")\n",
    "            else:\n",
    "                selected_common_df = pd.DataFrame()\n",
    "                print(\"   Nenhum caso comum selecionado\")\n",
    "        else:\n",
    "            selected_common_df = pd.DataFrame()\n",
    "            print(\"   Nenhuma observa√ß√£o comum dispon√≠vel\")\n",
    "        \n",
    "        # Etapa 4: Combinar todos os casos\n",
    "        print(f\"\\nüîó Etapa 4: Combinando datasets...\")\n",
    "        \n",
    "        datasets_to_combine = []\n",
    "        \n",
    "        # Adicionar casos raros originais\n",
    "        if len(rare_df) > 0:\n",
    "            datasets_to_combine.append(rare_df)\n",
    "            print(f\"   - Casos raros originais: {len(rare_df):,}\")\n",
    "        \n",
    "        # Adicionar casos comuns selecionados\n",
    "        if len(selected_common_df) > 0:\n",
    "            datasets_to_combine.append(selected_common_df)\n",
    "            print(f\"   - Casos comuns selecionados: {len(selected_common_df):,}\")\n",
    "        \n",
    "        # Adicionar casos sint√©ticos\n",
    "        if len(synthetic_cases) > 0:\n",
    "            datasets_to_combine.append(synthetic_cases)\n",
    "            print(f\"   - Casos sint√©ticos: {len(synthetic_cases):,}\")\n",
    "        \n",
    "        if not datasets_to_combine:\n",
    "            print(\"‚ùå Nenhum dataset para combinar. Retornando dataset original.\")\n",
    "            return dataframe.copy()\n",
    "        \n",
    "        # Combinar datasets\n",
    "        with tqdm(total=1, desc=\"Combinando datasets\") as pbar:\n",
    "            final_dataframe = pd.concat(datasets_to_combine, ignore_index=True)\n",
    "            pbar.update(1)\n",
    "        \n",
    "        # Etapa 5: Relat√≥rio final\n",
    "        print(f\"\\n‚úÖ SMOTE-R conclu√≠do com sucesso!\")\n",
    "        print(f\"   Dataset original: {len(dataframe):,} registros\")\n",
    "        print(f\"   Dataset final: {len(final_dataframe):,} registros\")\n",
    "        print(f\"   Varia√ß√£o: {((len(final_dataframe) - len(dataframe)) / len(dataframe) * 100):+.1f}%\")\n",
    "        \n",
    "        # Estat√≠sticas do target\n",
    "        print(f\"\\nüìä Estat√≠sticas do target '{target_column}':\")\n",
    "        print(f\"   Original - M√©dia: {dataframe[target_column].mean():.3f}, Mediana: {dataframe[target_column].median():.3f}\")\n",
    "        print(f\"   Final    - M√©dia: {final_dataframe[target_column].mean():.3f}, Mediana: {final_dataframe[target_column].median():.3f}\")\n",
    "        \n",
    "        return final_dataframe\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro cr√≠tico no SMOTE-R: {e}\")\n",
    "        print(\"   Retornando dataset original...\")\n",
    "        return dataframe.copy()\n",
    "\n",
    "\n",
    "def generate_synthetic_cases_with_target_smoter(dataframe, target_column, explanatory_variables, \n",
    "                                               oversampling_ratio, num_neighbors, constraint_columns=None):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o auxiliar para gerar casos sint√©ticos otimizada para SMOTE-R\n",
    "    \n",
    "    Par√¢metros adicionais:\n",
    "    - constraint_columns: Lista de colunas que devem ter valores iguais entre vizinhos\n",
    "    \"\"\"\n",
    "    \n",
    "    synthetic_cases = []\n",
    "    num_new_cases = int(oversampling_ratio / 100)\n",
    "    \n",
    "    if num_new_cases == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Separar features e target\n",
    "    features = dataframe[explanatory_variables].copy()\n",
    "    target = dataframe[target_column].copy()\n",
    "    \n",
    "    # Separar colunas num√©ricas e categ√≥ricas das features\n",
    "    numeric_columns = features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_columns = features.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Preparar dados para KNN\n",
    "    label_encoders = {}\n",
    "    encoded_features = features.copy()\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        encoded_features[col] = le.fit_transform(features[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    features_for_knn = encoded_features.values\n",
    "    knn_model = NearestNeighbors(n_neighbors=min(num_neighbors + 1, len(dataframe)))\n",
    "    knn_model.fit(features_for_knn)\n",
    "    \n",
    "    # Para cada caso existente\n",
    "    with tqdm(total=len(features), desc=\"Gerando casos sint√©ticos\") as pbar:\n",
    "        for case_index, (_, original_case) in enumerate(features.iterrows()):\n",
    "            \n",
    "            # Encontrar vizinhos mais pr√≥ximos\n",
    "            distances, neighbor_indices = knn_model.kneighbors([features_for_knn[case_index]])\n",
    "            neighbor_indices = neighbor_indices[0][1:]  # Remove o primeiro (pr√≥prio caso)\n",
    "            \n",
    "            # Gerar novos casos\n",
    "            for _ in range(num_new_cases):\n",
    "                selected_neighbor_idx = random.choice(neighbor_indices)\n",
    "                selected_neighbor = features.iloc[selected_neighbor_idx]\n",
    "                \n",
    "                synthetic_case = {}\n",
    "                \n",
    "                # Processar features\n",
    "                for attribute_name in explanatory_variables:\n",
    "                    if attribute_name in numeric_columns:\n",
    "                        original_value = original_case[attribute_name]\n",
    "                        neighbor_value = selected_neighbor[attribute_name]\n",
    "                        difference = neighbor_value - original_value\n",
    "                        random_factor = np.random.uniform(0, 1)\n",
    "                        synthetic_value = original_value + random_factor * difference\n",
    "                        synthetic_case[attribute_name] = synthetic_value\n",
    "                    else:\n",
    "                        original_value = original_case[attribute_name]\n",
    "                        neighbor_value = selected_neighbor[attribute_name]\n",
    "                        synthetic_case[attribute_name] = random.choice([original_value, neighbor_value])\n",
    "                \n",
    "                # Calcular target usando m√©dia ponderada pelas dist√¢ncias\n",
    "                original_target = target.iloc[case_index]\n",
    "                neighbor_target = target.iloc[selected_neighbor_idx]\n",
    "                \n",
    "                # Usar dist√¢ncias para ponderar\n",
    "                case_distance = distances[0][0] if distances[0][0] > 0 else 0.001\n",
    "                neighbor_distance = distances[0][np.where(neighbor_indices == selected_neighbor_idx)[0][0] + 1]\n",
    "                \n",
    "                total_distance = case_distance + neighbor_distance\n",
    "                if total_distance > 0:\n",
    "                    weight_original = neighbor_distance / total_distance\n",
    "                    weight_neighbor = case_distance / total_distance\n",
    "                else:\n",
    "                    weight_original = weight_neighbor = 0.5\n",
    "                \n",
    "                synthetic_target = weight_original * original_target + weight_neighbor * neighbor_target\n",
    "                synthetic_case[target_column] = synthetic_target\n",
    "                \n",
    "                synthetic_cases.append(synthetic_case)\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Incluir todas as colunas originais (n√£o apenas explicativas)\n",
    "    result_df = pd.DataFrame(synthetic_cases)\n",
    "    \n",
    "    # Adicionar colunas que n√£o estavam nas vari√°veis explicativas nem target\n",
    "    missing_columns = set(dataframe.columns) - set(explanatory_variables) - {target_column}\n",
    "    for col in missing_columns:\n",
    "        # Para colunas n√£o explicativas, usar valores da primeira linha como padr√£o\n",
    "        # ou implementar l√≥gica espec√≠fica conforme necess√°rio\n",
    "        result_df[col] = dataframe[col].iloc[0]  # Simplifica√ß√£o - pode ser melhorado\n",
    "    \n",
    "    # Reordenar colunas para manter ordem original\n",
    "    result_df = result_df[dataframe.columns]\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d2564235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Iniciando SMOTE-R...\n",
      "   Dataset original: 834,592 registros\n",
      "   Usando 2 vari√°veis explicativas especificadas\n",
      "   Restri√ß√µes de vizinhan√ßa: ['dt_medicao']\n",
      "\n",
      "üîç Etapa 1: Identificando observa√ß√µes raras e comuns...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef153466419417b9c62930d7bcfa2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculando relev√¢ncia:   0%|          | 0/834592 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Observa√ß√µes raras: 101,872 (12.2%)\n",
      "   Observa√ß√µes comuns: 732,720 (87.8%)\n",
      "\n",
      "üß¨ Etapa 2: Gerando casos sint√©ticos (oversampling: 150%)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e4d224e15a4a8eaffc4177722a9a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gerando casos sint√©ticos:   0%|          | 0/101872 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Casos sint√©ticos gerados: 101,872\n",
      "\n",
      "‚öñÔ∏è Etapa 3: Sub-amostragem de observa√ß√µes comuns (undersampling: 80%)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94064d7c570847a2b19f8e07c9482765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Selecionando casos comuns:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Casos comuns selecionados: 162,995 de 732,720\n",
      "\n",
      "üîó Etapa 4: Combinando datasets...\n",
      "   - Casos raros originais: 101,872\n",
      "   - Casos comuns selecionados: 162,995\n",
      "   - Casos sint√©ticos: 101,872\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34248d72ef384c4fb3c81820eac56ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combinando datasets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ SMOTE-R conclu√≠do com sucesso!\n",
      "   Dataset original: 834,592 registros\n",
      "   Dataset final: 366,739 registros\n",
      "   Varia√ß√£o: -56.1%\n",
      "\n",
      "üìä Estat√≠sticas do target 'vl_precipitacao':\n",
      "   Original - M√©dia: 3.944, Mediana: 0.000\n",
      "   Final    - M√©dia: 15.159, Mediana: 11.400\n"
     ]
    }
   ],
   "source": [
    "df_balanced = smoteR(\n",
    "    dataframe=df,\n",
    "    target_column='vl_precipitacao',\n",
    "    explanatory_variables=['latitude', 'longitude'],\n",
    "    constraint_columns='dt_medicao',  # String √∫nica\n",
    "    pct_oversampling=150,\n",
    "    pct_undersampling=80,\n",
    "    number_of_nearest_neighbors=3,\n",
    "    random_state=42,\n",
    "    relevance_function=relevance_function\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
